#!/bin/bash
#PBS -A wff3_b_g_bc_default
#PBS -l nodes=1:ppn=8
#PBS -l pmem=3gb
#PBS -l walltime=1:00:00
#PBS -j oe
#PBS -o analysis.log
#PBS -N analysis

cd ${PBS_O_WORKDIR}

echo Job analysis has started on $(hostname) at $(date)
module load gcc/5.3.1
module load parallel/20170522
module load python/3.6.3-anaconda5.0.1

# Activate the python_gnu_parallel environment and set up the PATH
source activate python_gnu_parallel
export PATH=${HOME}/work/python_gnu_parallel_test_aci/bin:${PATH}

if [ ! -d ${HOME}/scratch/tmp ]; then
    mkdir -p ${HOME}/scratch/tmp
fi

# Create the fragments in ~/scratch/tmp
fragmentor -i ${HOME}/work/python_gnu_parallel_test_aci/data/data.csv -f 4 -p ~/scratch/tmp/

# Functions to be used with GNU parallel
function analysis_1 () {
    analysis -i $1 -c determine_most_popular_email
}

function analysis_2 () {
    analysis -i $1 -c determine_most_popular_free_email
}

function analysis_3 () {
    analysis -i $1 -c determine_most_popular_document_format
}

function analysis_4 () {
    analysis -i $1 -c determine_most_popular_audio_format
}

function analysis_5 () {
    analysis -i $1 -c determine_most_popular_image_format
}

# Now run the analyses using GNU parallel. Keep in same order. Output to .xml files
cd ${HOME}/scratch/tmp
parallel -k analysis_1 ::: fragment_1.csv fragment_2.csv fragment_3.csv fragment_4.csv >> \
    ./most_popular_email.xml
parallel -k analysis_2 ::: fragment_1.csv fragment_2.csv fragment_3.csv fragment_4.csv >> \
    ./most_popular_free_email.xml
parallel -k analysis_3 ::: fragment_1.csv fragment_2.csv fragment_3.csv fragment_4.csv >> \
    ./most_popular_document_format.xml
parallel -k analysis_4 ::: fragment_1.csv fragment_2.csv fragment_3.csv fragment_4.csv >> \
    ./most_popular_audio_format.xml
parallel -k analysis_5 ::: fragment_1.csv fragment_2.csv fragment_3.csv fragment_4.csv >> \
    ./most_popular_image_format.xml

# Generate graphs and save to scratch

echo Job analysis ended at $(date)
